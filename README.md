# LLMs



1. RedPajama: https://www.storminthecastle.com/posts/finetune_redpajama/
2. Accelerating LLaMA with Fabric: https://lightning.ai/pages/community/tutorial/accelerating-llama-with-fabric-a-comprehensive-guide-to-training-and-fine-tuning-llama/
3. How To Fine-Tune LLaMA, OpenLLaMA, And XGen, With JAX On A GPU Or A TPU: https://nlpcloud.com/how-to-fine-tune-llama-openllama-xgen-with-jax-on-tpu-gpu.html
4. Train LLaMA with RLHF (StackLLaMA): https://huggingface.co/blog/stackllama 



# Note:

1. Various PEFT techniques include Prefix Tuning, Low-Rank adaptation (LoRA), and the insertion of adapter layers in pretrained large language models.
2. Prefix Tuning appends a collection of prefixes to autoregressive language models, or alternatively, incorporates prefixes for both encoder and decoder components.
3. LoRA introduces trainable rank decomposition matrices into each layer.
4. Adapter involves inserting lightweight modules into each layer of pretrained models, with only the adapters being trained during fine-tuning


# Interesting Reads

1. LLaMA-Adapter : https://lupantech.github.io/papers/arxiv23_llama.pdf


![image](https://github.com/DrishtiShrrrma/LLMs/assets/129742046/de0af1b6-5966-4abf-a947-248f97f870b3)

